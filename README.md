# Evaluating-Pre-Trained-Models-for-Physical-Commonsense-RoBERTa-vs.-GPT-2

## Project Overview
This project compares **RoBERTa** and **GPT-2** on the **PiQA** benchmark, testing their ability to reason about physical commonsense. The models are evaluated on multiple-choice questions where they choose between two possible answers. We compare performance using **Accuracy**, with GPT-2 evaluated based on a **loss comparison** between answers.

## Data Source
- **PiQA Dataset**: A dataset with questions on physical commonsense. Available on [Hugging Face](https://huggingface.co/datasets/piqa).

## Models
- **RoBERTa**: `roberta-large` 
- **GPT-2**: `gpt2`

## Instructions
2. **Run the File**: 
    - Execute `NLP_P3.ipynb` for model evaluation and comparison.

## Refer to the Documentation
- Detailed explanations, methodologies, and results are in the project documentation within the notebook.
